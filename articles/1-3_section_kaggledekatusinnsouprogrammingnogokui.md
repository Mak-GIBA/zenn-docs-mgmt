---
title: "1-3章纏め: kaggleに挑む深層プログラミングの極意"
emoji: "📚"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---
「[kaggleに挑む深層プログラミングの極意](https://www.amazon.co.jp/Kaggle%E3%81%AB%E6%8C%91%E3%82%80%E6%B7%B1%E5%B1%A4%E5%AD%A6%E7%BF%92%E3%83%97%E3%83%AD%E3%82%B0%E3%83%A9%E3%83%9F%E3%83%B3%E3%82%B0%E3%81%AE%E6%A5%B5%E6%84%8F-KS%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E5%B0%8F%E5%B5%9C-%E8%80%95%E5%B9%B3/dp/4065305136)」を読んだので，本書の1-3章で自分が役立つと感じた情報を纏めました。

# NN以外のアルゴリズム
## 勾配ブースティング決定木
- 勾配ブースティング決定木の学習では，ある時点で作成した決定木の予測結果を確認して誤差の大きいデータをうまく予測できるように次の決定木を作成していく。

# NNの高速化/安定化
- 実環境ではNNの学習/推論速度も重要。
- [混合精度 (mixed precision)](https://qiita.com/MotonobuHommi/items/f12a500d6c475ce59790#3-mixed-precision): 加納な限り数値表現のデータ量を節約しつつ，一方で数値誤差によるモデルの性能悪化ができるだけ起きないようにする方法 (アーキテクチャなどを変えずに計算速度やメモリの使用量を改善する方法)。行列計算の一部の演算で16ビットを利用することで，計算速度を高速化する。メモリ削除により大きなバッチサイズを確保でき，学習の安定化にも繋がる。　
![alt text](image.png)

- 勾配累積 (gradient accumulation): 小さいバッチサイズでも学習を安定させる手法。複数の小さいバッチそれぞれで勾配を分けて計算し，その平均を用いてモデルのパラメータを更新する。累積回数を乗じた分のバッチサイズを確保した場合と同様の効果が期待できる。
- [Pytorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)は一読の価値あり。

# NNのモデル作成時の留意点
- NNの実装を効率的に進めるためには，できる限り手戻りが少ないように順を追って挙動を確認するのが良い。以下は戦略の一例[参考文献: https://fullstackdeeplearning.com/spring2021/lecture-7/]。
  1. **単純なアーキテクチャや学習設定からはじめる**:
  モデルの学習が問題なくはじまることを確認する。最初の段階ではデータセットの一部のみを抽出することで読み込み時間を短縮し，データセットの形状の不一致などが発生せず，モデルの学習が進行するか確かめる。モデルの出力がNanになっていないか確認。少量データセットで確認できた後は全量データセットでも確認し，メモリ不足エラーになったら計算資源に適したバッチサイズに確認して設定をチューニング。
  2. **訓練データセットで学習できると確認する**:
  モデルが最低限動いたら次は学習で損失が問題なく下がることを確認する。NNの学習率は大きすぎると目的関数が発散したり，小さすぎると全く学習が進まなかったりする ($10^{-6}～10^{-3}$であることが多い)。
  3. 検証データセットに対する汎化性能を確認する